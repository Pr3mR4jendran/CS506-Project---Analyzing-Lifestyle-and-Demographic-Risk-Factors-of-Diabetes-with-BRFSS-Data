{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c4fd72-d0b9-4bb6-95b6-6f5f486a11a5",
   "metadata": {},
   "source": [
    "## DATA CLEANING & FEATURE SELECTION\n",
    "## Using Chi-Square test, ANOVA / Kruskal-H Test, Pearson / Spearman Correlation\n",
    "#### Saneeya Vichare & Jyoti Shree\n",
    "\n",
    "#### October 23rd\n",
    "\n",
    "### This involves:\n",
    "1. Removing all rows belonging to refused or blank categories (7, 9, 99, BLANK, etc.)\n",
    "2. Removing columns which have more than 30% missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80f670-1344-447b-8060-3e6a0a7c36d2",
   "metadata": {},
   "source": [
    "### *STEP 1: DATA LOADING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1ea5e7e-8033-40e3-860d-99ea0b8a264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1l/dzhvlzcx0k57wvcl_4q0dhlc0000gn/T/ipykernel_4738/2136090270.py:5: DtypeWarning: Columns (288) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"BRFSS_2024.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"BRFSS_2024.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa2fab7-6e10-40a7-b8b1-d5afc4f65d1a",
   "metadata": {},
   "source": [
    "### *STEP 2: REMOVING BLANK CATEGORIES*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae99eb50-f31c-4857-b1ce-25cecb6a6c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE CLEANING ===\n",
      "Original shape: (457670, 301)\n",
      "Initial total NaN values: 66,621,255\n",
      "\n",
      "Removed 3,395 rows where DIABETE4 == 2\n",
      "=== AFTER CLEANING ===\n",
      "New shape: (454275, 296)\n",
      "Total NaN values after cleaning: 66,916,108\n",
      "New NaN values introduced during cleaning: 294,853\n",
      "\n",
      "Columns dropped because they were 100% NaN (5 total):\n",
      "['HPVDSHT', 'ICFQSTVR', 'LCSLAST', 'RCSBORG1', '_AIDTST4']\n",
      "\n",
      "Top 10 columns with most missing values after cleaning:\n",
      "\n",
      "COLGHOUS    0.999971\n",
      "CSRVCTL2    0.998765\n",
      "CCLGHOUS    0.996872\n",
      "CSRVINST    0.994605\n",
      "NOBCUSE8    0.994497\n",
      "LASTSIG4    0.994486\n",
      "HPVADSH1    0.993823\n",
      "CSRVDOC1    0.992927\n",
      "CSRVSUM     0.992769\n",
      "CSRVRTRN    0.992399\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"=== BEFORE CLEANING ===\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "initial_missing = df.isnull().sum().sum()\n",
    "print(f\"Initial total NaN values: {initial_missing:,}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Replace CDC invalid numeric codes with NaN across ALL columns\n",
    "invalid_codes = [7, 9, 77, 99, 777, 999, 98, 9999]\n",
    "df = df.replace(invalid_codes, np.nan)\n",
    "\n",
    "# Replace empty or whitespace-only strings with NaN\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "# Remove rows where DIABETE4 == 2 (gestational diabetes)\n",
    "if 'DIABETE4' in df.columns:\n",
    "    before_drop = df.shape[0]\n",
    "    df = df[df['DIABETE4'] != 2]\n",
    "    after_drop = df.shape[0]\n",
    "    print(f\"Removed {before_drop - after_drop:,} rows where DIABETE4 == 2\")\n",
    "\n",
    "# Drop rows that have ALL NaN values (but NOT those with just some NaNs)\n",
    "df_cleaned = df.dropna(how='all')\n",
    "\n",
    "# Drop columns that are completely empty (100% NaN)\n",
    "cols_before = set(df.columns)\n",
    "df_cleaned = df_cleaned.dropna(axis=1, how='all')\n",
    "cols_after = set(df_cleaned.columns)\n",
    "dropped_cols = cols_before - cols_after\n",
    "\n",
    "\n",
    "\n",
    "# Summary of cleaning results\n",
    "print(\"=== AFTER CLEANING ===\")\n",
    "print(f\"New shape: {df_cleaned.shape}\")\n",
    "final_missing = df_cleaned.isnull().sum().sum()\n",
    "new_nans = final_missing - initial_missing\n",
    "print(f\"Total NaN values after cleaning: {final_missing:,}\")\n",
    "print(f\"New NaN values introduced during cleaning: {new_nans:,}\\n\")\n",
    "\n",
    "if dropped_cols:\n",
    "    print(f\"Columns dropped because they were 100% NaN ({len(dropped_cols)} total):\")\n",
    "    print(sorted(list(dropped_cols)))\n",
    "else:\n",
    "    print(\"No columns were fully NaN — none dropped.\")\n",
    "\n",
    "# (Optional) Show top 10 columns with most missing values after cleaning\n",
    "print(\"\\nTop 10 columns with most missing values after cleaning:\\n\")\n",
    "print(df_cleaned.isnull().mean().sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44275438-cfaa-4059-b701-3bfcb049439b",
   "metadata": {},
   "source": [
    "### *STEP 3: Remove columns with more than 30% missing values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "115141d6-e765-4286-ab4a-7c8f1e21709c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COLUMN THRESHOLD CLEANING (Step 2) ===\n",
      "Initial columns: 296\n",
      "Columns removed (>30% missing): 177\n",
      "Remaining columns: 119\n",
      "\n",
      "List of removed columns:\n",
      "\n",
      "- CTELENM1\n",
      "- PVTRESD1\n",
      "- COLGHOUS\n",
      "- STATERE1\n",
      "- CELPHON1\n",
      "- LADULT1\n",
      "- NUMADULT\n",
      "- RESPSLC1\n",
      "- LANDSEX3\n",
      "- CCLGHOUS\n",
      "- POORHLTH\n",
      "- ASTHNOW\n",
      "- DIABAGE4\n",
      "- NUMHHOL4\n",
      "- NUMPHON4\n",
      "- EMPLOY1\n",
      "- INCOME3\n",
      "- PREGNANT\n",
      "- HADMAM\n",
      "- HOWLONG\n",
      "- CERVSCRN\n",
      "- CRVCLCNC\n",
      "- CRVCLPAP\n",
      "- CRVCLHPV\n",
      "- HADHYST2\n",
      "- HADSIGM4\n",
      "- COLNSIGM\n",
      "- COLNTES1\n",
      "- SIGMTES1\n",
      "- LASTSIG4\n",
      "- COLNCNCR\n",
      "- VIRCOLO1\n",
      "- VCLNTES2\n",
      "- SMALSTOL\n",
      "- STOLTEST\n",
      "- STOOLDN2\n",
      "- BLDSTFIT\n",
      "- SDNATES1\n",
      "- SMOKDAY2\n",
      "- LCSFIRST\n",
      "- LCSNUMCG\n",
      "- LCSSCNCR\n",
      "- LCSCTWHN\n",
      "- AVEDRNK4\n",
      "- DRNK3GE5\n",
      "- MAXDRNKS\n",
      "- FLSHTMY3\n",
      "- IMFVPLA5\n",
      "- HIVTSTD3\n",
      "- PDIABTS1\n",
      "- PREDIAB2\n",
      "- DIABTYPE\n",
      "- INSULIN1\n",
      "- CHKHEMO3\n",
      "- EYEEXAM1\n",
      "- DIABEYE1\n",
      "- DIABEDU1\n",
      "- FEETSORE\n",
      "- ARTHEXER\n",
      "- SHINGLE2\n",
      "- HPVADVC4\n",
      "- HPVADSH1\n",
      "- TETANUS1\n",
      "- CNCRDIFF\n",
      "- CNCRAGE\n",
      "- CNCRTYP2\n",
      "- CSRVTRT3\n",
      "- CSRVDOC1\n",
      "- CSRVSUM\n",
      "- CSRVRTRN\n",
      "- CSRVINST\n",
      "- CSRVINSR\n",
      "- CSRVDEIN\n",
      "- CSRVCLIN\n",
      "- CSRVPAIN\n",
      "- CSRVCTL2\n",
      "- PSATEST1\n",
      "- PSATIME1\n",
      "- PCPSARS2\n",
      "- PSASUGS1\n",
      "- PCSTALK2\n",
      "- CIMEMLO1\n",
      "- CDWORRY\n",
      "- CDDISCU1\n",
      "- CDHOUS1\n",
      "- CDSOCIA1\n",
      "- CAREGIV1\n",
      "- CRGVREL5\n",
      "- CRGVPRB4\n",
      "- CRGVALZD\n",
      "- CRGVNURS\n",
      "- CRGVPER2\n",
      "- CRGVHOU2\n",
      "- CRGVHRS2\n",
      "- CRGVLNG2\n",
      "- ACEDEPRS\n",
      "- ACEDRINK\n",
      "- ACEDRUGS\n",
      "- ACEPRISN\n",
      "- ACEDIVRC\n",
      "- ACEPUNCH\n",
      "- ACEHURT1\n",
      "- ACESWEAR\n",
      "- ACETOUCH\n",
      "- ACETTHEM\n",
      "- ACEHVSEX\n",
      "- ACEADSAF\n",
      "- ACEADNED\n",
      "- LSATISFY\n",
      "- EMTSUPRT\n",
      "- SDLONELY\n",
      "- SDHEMPLY\n",
      "- FOODSTMP\n",
      "- SDHFOOD1\n",
      "- SDHBILLS\n",
      "- SDHUTILS\n",
      "- SDHTRNSP\n",
      "- HOWSAFE1\n",
      "- MARIJAN1\n",
      "- MARJSMOK\n",
      "- MARJEAT\n",
      "- MARJVAPE\n",
      "- MARJDAB\n",
      "- MARJOTHR\n",
      "- USEMRJN4\n",
      "- LASTSMK2\n",
      "- STOPSMK2\n",
      "- MENTCIGS\n",
      "- MENTECIG\n",
      "- HEATTBCO\n",
      "- SSBSUGR2\n",
      "- SSBFRUT3\n",
      "- FIREARM5\n",
      "- GUNLOAD\n",
      "- LOADULK2\n",
      "- RCSRLTN2\n",
      "- CASTHDX2\n",
      "- CASTHNO2\n",
      "- SOMALE\n",
      "- SOFEMALE\n",
      "- HADSEX\n",
      "- PFPPRVN4\n",
      "- TYPCNTR9\n",
      "- NOBCUSE8\n",
      "- MSCODE\n",
      "- _CHISPNC\n",
      "- _CRACE1\n",
      "- CAGEG\n",
      "- _CLLCPWT\n",
      "- _DUALUSE\n",
      "- _DUALCOR\n",
      "- _HCVU654\n",
      "- _ALTETH3\n",
      "- _RFMAM23\n",
      "- _MAM402Y\n",
      "- _CRVSCRN\n",
      "- _RFPAP37\n",
      "- _HPV5YR1\n",
      "- _PAPHPV1\n",
      "- _HADCOLN\n",
      "- _CLNSCP2\n",
      "- _HADSIGM\n",
      "- _SGMSCP2\n",
      "- _SGMS102\n",
      "- _RFBLDS6\n",
      "- _STOLDN2\n",
      "- _VIRCOL2\n",
      "- _SBONTI2\n",
      "- _CRCREC3\n",
      "- LCSFIRS_\n",
      "- LCSLAST_\n",
      "- _LCSAGE\n",
      "- _LCSYSMK\n",
      "- _PACKDAY\n",
      "- _PACKYRS\n",
      "- _LCSSMKG\n",
      "- _RFDRHV9\n",
      "\n",
      "Top 10 columns with most missing values (remaining data):\n",
      "\n",
      "_INCOMG1    0.255971\n",
      "HHADULT     0.189465\n",
      "CELLSEX3    0.188945\n",
      "FMONTH      0.187965\n",
      "LANDLINE    0.181441\n",
      "CSTATE1     0.179052\n",
      "CADULT1     0.179052\n",
      "CELLFON5    0.179050\n",
      "PVTRESD3    0.179048\n",
      "CTELNUM1    0.179048\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define threshold\n",
    "threshold = 0.3  # 30%\n",
    "\n",
    "# Calculate missing percentage per column\n",
    "missing_percent = df_cleaned.isnull().mean()\n",
    "\n",
    "# Identify columns to drop\n",
    "cols_to_drop = missing_percent[missing_percent > threshold].index.tolist()\n",
    "\n",
    "# Drop those columns\n",
    "df_thresholded = df_cleaned.drop(columns=cols_to_drop)\n",
    "\n",
    "# Print summary\n",
    "print(\"=== COLUMN THRESHOLD CLEANING (Step 2) ===\")\n",
    "print(f\"Initial columns: {df_cleaned.shape[1]}\")\n",
    "print(f\"Columns removed (>30% missing): {len(cols_to_drop)}\")\n",
    "print(f\"Remaining columns: {df_thresholded.shape[1]}\\n\")\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(\"List of removed columns:\\n\")\n",
    "    for c in cols_to_drop:\n",
    "        print(\"-\", c)\n",
    "else:\n",
    "    print(\"No columns exceeded 30% missing — none removed.\")\n",
    "\n",
    "# (Optional) Quick sanity check\n",
    "print(\"\\nTop 10 columns with most missing values (remaining data):\\n\")\n",
    "print(df_thresholded.isnull().mean().sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3ae40-1a11-4198-85eb-cc8ca7bf9c01",
   "metadata": {},
   "source": [
    "### *STEP 4: DEFINING VARIABLE TYPES*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06804472-2a99-4399-a939-0f4dee87ea55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical-like columns: 93\n",
      "Continuous-like columns: 26\n",
      "_BMI5: CONTINUOUS\n",
      "_BMI5CAT: CATEGORICAL\n",
      "_RFBMI5: CATEGORICAL\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical-like numeric columns\n",
    "categorical_like = [col for col in df_thresholded.columns if df_thresholded[col].nunique() <= 15]\n",
    "\n",
    "# Everything else is continuous/numeric\n",
    "continuous_like = [col for col in df_thresholded.columns if df_thresholded[col].nunique() > 15]\n",
    "\n",
    "print(f\"Categorical-like columns: {len(categorical_like)}\")\n",
    "print(f\"Continuous-like columns: {len(continuous_like)}\")\n",
    "\n",
    "bmi_cols = ['_BMI5', '_BMI5CAT', '_RFBMI5']\n",
    "\n",
    "for col in bmi_cols:\n",
    "    if col in categorical_like:\n",
    "        print(f\"{col}: CATEGORICAL\")\n",
    "    elif col in continuous_like:\n",
    "        print(f\"{col}: CONTINUOUS\")\n",
    "    else:\n",
    "        print(f\"{col}: NOT FOUND in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc36536-559a-4204-be11-cb2d967eff97",
   "metadata": {},
   "source": [
    "### *STEP 5: STATISTICAL TESTING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "383b4d6d-1bcd-4ced-843b-8d09535eb4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# =========================================\n",
    "# CONFIGURATION\n",
    "# =========================================\n",
    "TARGET = 'DIABETE4'\n",
    "VALID_TARGET_CLASSES = sorted(df[TARGET].dropna().unique())  # e.g., [1, 3, 4]\n",
    "categorical_cols = categorical_like   # from your earlier step\n",
    "continuous_cols = continuous_like\n",
    "\n",
    "# Containers for results\n",
    "cat_results, cont_results = [], []\n",
    "\n",
    "# =========================================\n",
    "# CATEGORICAL FEATURES → Chi-Square test\n",
    "# =========================================\n",
    "for col in categorical_cols:\n",
    "    sub = df[[col, TARGET]].dropna()\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    # Ensure both columns are 1D and contain clean scalar values\n",
    "    try:\n",
    "        x = pd.Series(sub[col].astype(str).values.ravel(), name=col)\n",
    "        y = pd.Series(sub[TARGET].astype(str).values.ravel(), name=TARGET)\n",
    "        tbl = pd.crosstab(x, y)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {col}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if tbl.shape[0] < 2 or tbl.shape[1] < 2:\n",
    "        continue\n",
    "    try:\n",
    "        chi2_stat, p, dof, exp = stats.chi2_contingency(tbl, correction=False)\n",
    "        cat_results.append({\n",
    "            \"feature\": col,\n",
    "            \"test\": \"chi2\",\n",
    "            \"statistic\": chi2_stat,\n",
    "            \"p_value\": p\n",
    "        })\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# =======================================================================\n",
    "# CONTINUOUS FEATURES → ANOVA / Kruskal-H Test + Pearson / Spearman Correlation\n",
    "# =======================================================================\n",
    "for col in continuous_cols:\n",
    "    sub = df[[col, TARGET]].dropna()\n",
    "    if sub.empty:\n",
    "        continue\n",
    "\n",
    "    # ---- ANOVA / Kruskal ----\n",
    "    groups = [sub.loc[sub[TARGET]==cls, col] for cls in VALID_TARGET_CLASSES]\n",
    "    try:\n",
    "        fstat, p = stats.f_oneway(*groups)\n",
    "        cont_results.append({\n",
    "            \"feature\": col,\n",
    "            \"test\": \"anova\",\n",
    "            \"statistic\": fstat,\n",
    "            \"p_value\": p\n",
    "        })\n",
    "    except Exception:\n",
    "        # fallback: Kruskal-Wallis (non-parametric)\n",
    "        try:\n",
    "            stat, p = stats.kruskal(*groups, nan_policy=\"omit\")\n",
    "            cont_results.append({\n",
    "                \"feature\": col,\n",
    "                \"test\": \"kruskal\",\n",
    "                \"statistic\": stat,\n",
    "                \"p_value\": p\n",
    "            })\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ---- Pearson / Spearman Correlation ----\n",
    "    try:\n",
    "        pearson_r, pearson_p = stats.pearsonr(sub[col], sub[TARGET])\n",
    "        cont_results.append({\n",
    "            \"feature\": col,\n",
    "            \"test\": \"pearson\",\n",
    "            \"statistic\": pearson_r,\n",
    "            \"p_value\": pearson_p\n",
    "        })\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        spearman_r, spearman_p = stats.spearmanr(sub[col], sub[TARGET])\n",
    "        cont_results.append({\n",
    "            \"feature\": col,\n",
    "            \"test\": \"spearman\",\n",
    "            \"statistic\": spearman_r,\n",
    "            \"p_value\": spearman_p\n",
    "        })\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# COMBINE & ADJUST P-VALUES\n",
    "# =========================================\n",
    "\n",
    "results_df = pd.concat([\n",
    "    pd.DataFrame(cat_results),\n",
    "    pd.DataFrame(cont_results)\n",
    "], ignore_index=True).dropna(subset=['p_value'])\n",
    "\n",
    "# Multiple test correction (Benjamini–Hochberg FDR)\n",
    "_, p_adj, _, _ = multipletests(results_df['p_value'], method='fdr_bh')\n",
    "results_df['p_adj'] = p_adj\n",
    "\n",
    "# Add significance flag\n",
    "results_df['significant'] = np.where(results_df['p_adj'] < 0.05, 'Yes', 'No')\n",
    "\n",
    "# Sort by adjusted p-value\n",
    "results_df = results_df.sort_values('p_adj').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c61ca07-1fc5-499c-b181-6bf36e04dc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most significant features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>test</th>\n",
       "      <th>statistic</th>\n",
       "      <th>p_value</th>\n",
       "      <th>p_adj</th>\n",
       "      <th>significant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QSTVER</td>\n",
       "      <td>chi2</td>\n",
       "      <td>3742.602755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_RACE</td>\n",
       "      <td>chi2</td>\n",
       "      <td>2649.123062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_AGE80</td>\n",
       "      <td>pearson</td>\n",
       "      <td>-0.198525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_MRACE1</td>\n",
       "      <td>chi2</td>\n",
       "      <td>2047.201241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_DRDXAR2</td>\n",
       "      <td>chi2</td>\n",
       "      <td>14092.651698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>_ASTHMS1</td>\n",
       "      <td>chi2</td>\n",
       "      <td>1779.725113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>_CASTHM1</td>\n",
       "      <td>chi2</td>\n",
       "      <td>1748.276259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>_MICHD</td>\n",
       "      <td>chi2</td>\n",
       "      <td>14653.108837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>_EXTETH3</td>\n",
       "      <td>chi2</td>\n",
       "      <td>14109.452154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>_RACEGR3</td>\n",
       "      <td>chi2</td>\n",
       "      <td>2233.933442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>_TOTINDA</td>\n",
       "      <td>chi2</td>\n",
       "      <td>9708.689460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>_RFHLTH</td>\n",
       "      <td>chi2</td>\n",
       "      <td>23662.629823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>_IMPRACE</td>\n",
       "      <td>chi2</td>\n",
       "      <td>2609.493490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>_RAWRAKE</td>\n",
       "      <td>chi2</td>\n",
       "      <td>1714.999648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>_AGE80</td>\n",
       "      <td>spearman</td>\n",
       "      <td>-0.177550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WTKG3</td>\n",
       "      <td>anova</td>\n",
       "      <td>5787.446403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WTKG3</td>\n",
       "      <td>pearson</td>\n",
       "      <td>-0.140031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PHYSHLTH</td>\n",
       "      <td>pearson</td>\n",
       "      <td>0.080620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>WTKG3</td>\n",
       "      <td>spearman</td>\n",
       "      <td>-0.124370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>_PHYS14D</td>\n",
       "      <td>chi2</td>\n",
       "      <td>10622.546156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature      test     statistic  p_value  p_adj significant\n",
       "0     QSTVER      chi2   3742.602755      0.0    0.0         Yes\n",
       "1      _RACE      chi2   2649.123062      0.0    0.0         Yes\n",
       "2     _AGE80   pearson     -0.198525      0.0    0.0         Yes\n",
       "3    _MRACE1      chi2   2047.201241      0.0    0.0         Yes\n",
       "4   _DRDXAR2      chi2  14092.651698      0.0    0.0         Yes\n",
       "5   _ASTHMS1      chi2   1779.725113      0.0    0.0         Yes\n",
       "6   _CASTHM1      chi2   1748.276259      0.0    0.0         Yes\n",
       "7     _MICHD      chi2  14653.108837      0.0    0.0         Yes\n",
       "8   _EXTETH3      chi2  14109.452154      0.0    0.0         Yes\n",
       "9   _RACEGR3      chi2   2233.933442      0.0    0.0         Yes\n",
       "10  _TOTINDA      chi2   9708.689460      0.0    0.0         Yes\n",
       "11   _RFHLTH      chi2  23662.629823      0.0    0.0         Yes\n",
       "12  _IMPRACE      chi2   2609.493490      0.0    0.0         Yes\n",
       "13  _RAWRAKE      chi2   1714.999648      0.0    0.0         Yes\n",
       "14    _AGE80  spearman     -0.177550      0.0    0.0         Yes\n",
       "15     WTKG3     anova   5787.446403      0.0    0.0         Yes\n",
       "16     WTKG3   pearson     -0.140031      0.0    0.0         Yes\n",
       "17  PHYSHLTH   pearson      0.080620      0.0    0.0         Yes\n",
       "18     WTKG3  spearman     -0.124370      0.0    0.0         Yes\n",
       "19  _PHYS14D      chi2  10622.546156      0.0    0.0         Yes"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Significant features by test type:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test\n",
       "chi2        87\n",
       "anova       25\n",
       "pearson     22\n",
       "spearman    22\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================\n",
    "# DISPLAY RESULTS\n",
    "# =========================================\n",
    "print(\"Top 20 most significant features:\")\n",
    "display(results_df.head(20))\n",
    "\n",
    "# Optional: summary counts\n",
    "print(\"\\nSignificant features by test type:\")\n",
    "display(results_df[results_df['significant']==\"Yes\"]['test'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68257364-d167-4f64-9d8f-b87dd2d80a10",
   "metadata": {},
   "source": [
    "### *STEP 6: Feature Importance and extraction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1b43d56-e2f3-46ed-9b75-7a017b5cbf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE IMPORTANCE \n",
    "\n",
    "\n",
    "# Convert adjusted p-values to -log10 importance\n",
    "results_df[\"importance_score\"] = -np.log10(results_df[\"p_adj\"].clip(lower=1e-300))\n",
    "\n",
    "# Normalize to range [0, 1]\n",
    "results_df[\"importance_norm\"] = (\n",
    "    results_df[\"importance_score\"] - results_df[\"importance_score\"].min()\n",
    ") / (results_df[\"importance_score\"].max() - results_df[\"importance_score\"].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e933ec7-801d-4ac5-aa34-3fae495cc314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 96 features (τ=0.6)\n",
      "Top few: ['QSTVER', '_RACE', '_AGE80', '_MRACE1', '_DRDXAR2', '_ASTHMS1', '_CASTHM1', '_MICHD', '_EXTETH3', '_RACEGR3']\n"
     ]
    }
   ],
   "source": [
    "# SELECTING FEATURES BY THRESHOLD\n",
    "\n",
    "tau = 0.6   # threshold cutoff\n",
    "n = 50      # keep at least top 50\n",
    "\n",
    "selected_by_tau = results_df[results_df[\"importance_norm\"] >= tau][\"feature\"].tolist()\n",
    "\n",
    "# Ensure we keep at least top N even if threshold is strict\n",
    "if len(selected_by_tau) < n:\n",
    "    selected_by_tau = results_df.head(n)[\"feature\"].tolist()\n",
    "\n",
    "print(f\"Selected {len(selected_by_tau)} features (τ={tau})\")\n",
    "print(\"Top few:\", selected_by_tau[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0bdf0f0-1f49-4141-b204-9371fceac9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape: (454275, 97)\n"
     ]
    }
   ],
   "source": [
    "# CRATING CLEAN DATASET WITH THESE NEW FEATURES\n",
    "\n",
    "TARGET = 'DIABETE4'\n",
    "df_selected = df[selected_by_tau + [TARGET]].copy()\n",
    "\n",
    "print(\"Final shape:\", df_selected.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ee6e2-63e5-4524-9752-cfe1650635f4",
   "metadata": {},
   "source": [
    "### *STEP 7: Feature Encoding and Normalization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d6842-a532-430d-a255-c23a4bc218ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "X = pd.get_dummies(df_selected.drop(columns=[TARGET]), drop_first=True)\n",
    "y = df_selected[TARGET]\n",
    "\n",
    "# Normalize numeric columns\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
